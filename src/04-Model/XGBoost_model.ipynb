{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import data\n",
    "data_df_1 = pd.read_csv(\"../../data/training_validation_1.csv\")\n",
    "data_df_2 = pd.read_csv(\"../../data/training_validation_2.csv\")\n",
    "train_df = pd.concat([data_df_1, data_df_2], axis=0) # Merge data frames\n",
    "# train_df = data_df_2.filter(regex=\"vCnt|value\")\n",
    "\n",
    "train_df['runId'] = 1000 * train_df['class'] + train_df['run']\n",
    "\n",
    "labels = train_df['class']\n",
    "runs = train_df['runId']\n",
    "\n",
    "run_df = train_df[['class', 'runId']].copy()\n",
    "run_df.drop_duplicates(inplace=True)\n",
    "run_df.reset_index(inplace=True)\n",
    "del run_df['index']\n",
    "\n",
    "# del train_df['class']\n",
    "del train_df['run']\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = list(train_df.columns)\n",
    "sensor_list.remove('runId')\n",
    "sensor_list.remove('class')\n",
    "len(sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def create_sequence(sequence, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequence[i:end_ix]\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_run(df, ws):\n",
    "#     data_data = np.empty((0, ws * len(sensor_list))) # for 1D\n",
    "#     data_data = np.empty((0, ws, len(sensor_list))) # for 2D\n",
    "#     data_data = np.empty((0, len(sensor_list), ws)) # for 2D\n",
    "#     label_data = np.empty((0, 1))\n",
    "\n",
    "    sensors_df = df.filter(sensor_list)\n",
    "\n",
    "    # Calculate seq of windows_size len\n",
    "    seq = create_sequence(sensors_df.values, n_steps=ws)\n",
    "#     seq = np.transpose(seq, axes=(0, 2, 1))\n",
    "    seq_count = seq.shape[0]\n",
    "    seq = seq.reshape((seq_count, -1)) # for 1D\n",
    "\n",
    "    # add new seq to data_data array\n",
    "#     data_data = np.vstack((data_data, seq))\n",
    "\n",
    "    # Calculate RULS\n",
    "    labels = df['class'].values[:seq_count]\n",
    "\n",
    "    # add rul to rul_data array\n",
    "#     rul_data = np.vstack((rul_data, ruls))\n",
    "\n",
    "# TODO: What is RUL_Max in this context?\n",
    "\n",
    "#     print (\"Shape:\", seq.shape, labels.shape)\n",
    "    return seq, labels\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(100))\n",
    "\n",
    "len(create_sequence(l, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: X_t, X_tp1, y_t, y_tp1 should be calculated per run.  \n",
    "# TODO: Then should be merged into one X_t, X_tp1, y_t, y_tp1.\n",
    "def create_datasets(df, ws):\n",
    "    \n",
    "    run_list = df['runId'].unique()\n",
    "\n",
    "    X_df_list = []\n",
    "    y_df_list = []\n",
    "    \n",
    "    for r in run_list:\n",
    "        r_df = df[df['runId'] == r]\n",
    "#         print (\"--> r: \", r, r_df.shape)\n",
    "        sensor_data, label_data = create_dataset_for_run(r_df, ws)\n",
    "\n",
    "        # Post Processing for the model\n",
    "\n",
    "        # Padding for model input \n",
    "        padded_sensor_data = sensor_data.copy() #np.hstack((sensor_data, np.zeros((sensor_data.shape[0], 2)))) # for AE     \n",
    "\n",
    "        # Calculate X(t) and X(t+1) for model input/output \n",
    "        X_t = padded_sensor_data[:]\n",
    "\n",
    "        # Calculate y(t) and y(t+1) for model input/output \n",
    "        y_t = label_data[:]\n",
    "\n",
    "        X_df_list.append(pd.DataFrame(X_t))\n",
    "        y_df_list.append(pd.DataFrame(y_t))\n",
    "    \n",
    "    X_t = pd.concat(X_df_list, axis=0) # Merge data frames\n",
    "    y_t = pd.concat(y_df_list, axis=0) # Merge data frames\n",
    "\n",
    "    return X_t.values, y_t.values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = StratifiedKFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "    print (\"--> Fold: \", fold)\n",
    "    \n",
    "    training_runIds = run_df.loc[training_indices]['runId']\n",
    "    validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "    \n",
    "    X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "    X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "    X_train, y_train = create_datasets(X_train_df, 1)\n",
    "    X_val, y_val = create_datasets(X_val_df, 1)\n",
    "    \n",
    "    print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "    print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "    print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "    \n",
    "    model = XGBClassifier(verbose=False, use_label_encoder=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    m = np.argmax(model.feature_importances_)\n",
    "    print (sensor_list[m], model.feature_importances_[m])\n",
    "    \n",
    "    pred = model.predict(X_val)\n",
    "    \n",
    "    acc_val = accuracy_score(pred, y_val)\n",
    "    f1_val = f1_score(pred, y_val, average='weighted')\n",
    "    \n",
    "    acc_sum += acc_val\n",
    "    f1_sum += f1_val\n",
    "    \n",
    "    print (\"Fold:\", fold, \"ACC:\", acc_val, \"F1:\", f1_val)\n",
    "\n",
    "print ()\n",
    "print (\"Avg ACC:\", acc_sum / 4.0, \"Avg F1:\", f1_sum / 4.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "    print (\"--> Fold: \", fold)\n",
    "    \n",
    "    training_runIds = run_df.loc[training_indices]['runId']\n",
    "    validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "    \n",
    "    X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "    X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "    X_train, y_train = create_datasets(X_train_df, 3)\n",
    "    X_val, y_val = create_datasets(X_val_df, 3)\n",
    "    \n",
    "    print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "    print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "    print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "    \n",
    "    model = XGBClassifier(verbose=False, use_label_encoder=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    m = np.argmax(model.feature_importances_)\n",
    "    print (sensor_list[m], model.feature_importances_[m])\n",
    "    \n",
    "    pred = model.predict(X_val)\n",
    "    \n",
    "    acc_val = accuracy_score(pred, y_val)\n",
    "    f1_val = f1_score(pred, y_val, average='weighted')\n",
    "    \n",
    "    acc_sum += acc_val\n",
    "    f1_sum += f1_val\n",
    "    \n",
    "    print (\"Fold:\", fold, \"ACC:\", acc_val, \"F1:\", f1_val)\n",
    "\n",
    "print ()\n",
    "print (\"Avg ACC:\", acc_sum / 4.0, \"Avg F1:\", f1_sum / 4.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "    print (\"--> Fold: \", fold)\n",
    "    \n",
    "    training_runIds = run_df.loc[training_indices]['runId']\n",
    "    validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "    \n",
    "    X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "    X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "    X_train_df.fillna(method='backfill', inplace=True)\n",
    "    X_val_df.fillna(method='backfill', inplace=True)\n",
    "\n",
    "    X_train_df.fillna(-1, inplace=True)\n",
    "    X_val_df.fillna(-1, inplace=True)\n",
    "\n",
    "    X_train, y_train = create_datasets(X_train_df, 1)\n",
    "    X_val, y_val = create_datasets(X_val_df, 1)\n",
    "    \n",
    "    pca_model = PCA(n_components=25)\n",
    "    \n",
    "    X_train = pca_model.fit_transform(X_train)\n",
    "    X_val = pca_model.transform(X_val)\n",
    "\n",
    "    \n",
    "    print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "    print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "    print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "    \n",
    "    model = XGBClassifier(verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    m = np.argmax(model.feature_importances_)\n",
    "    print (sensor_list[m], model.feature_importances_[m])\n",
    "    \n",
    "    pred = model.predict(X_val)\n",
    "    \n",
    "    acc_val = accuracy_score(pred, y_val)\n",
    "    f1_val = f1_score(pred, y_val, average='weighted')\n",
    "    \n",
    "    acc_sum += acc_val\n",
    "    f1_sum += f1_val\n",
    "    \n",
    "    print (\"Fold:\", fold, \"ACC:\", acc_val, \"F1:\", f1_val)\n",
    "\n",
    "print ()\n",
    "print (\"Avg ACC:\", acc_sum / 4.0, \"Avg F1:\", f1_sum / 4.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "acc_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "    print (\"--> Fold: \", fold)\n",
    "    \n",
    "    training_runIds = run_df.loc[training_indices]['runId']\n",
    "    validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "    \n",
    "    X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "    X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "    X_train_df.fillna(method='backfill', inplace=True)\n",
    "    X_val_df.fillna(method='backfill', inplace=True)\n",
    "\n",
    "    X_train_df.fillna(-1, inplace=True)\n",
    "    X_val_df.fillna(-1, inplace=True)\n",
    "\n",
    "    X_train, y_train = create_datasets(X_train_df, 6)\n",
    "    X_val, y_val = create_datasets(X_val_df, 6)\n",
    "    \n",
    "    pca_model = PCA(n_components=25)\n",
    "    \n",
    "    X_train = pca_model.fit_transform(X_train)\n",
    "    X_val = pca_model.transform(X_val)\n",
    "\n",
    "    \n",
    "    print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "    print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "    print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "    \n",
    "    model = XGBClassifier(verbose=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    m = np.argmax(model.feature_importances_)\n",
    "    print (sensor_list[m], model.feature_importances_[m])\n",
    "    \n",
    "    pred = model.predict(X_val)\n",
    "    \n",
    "    acc_val = accuracy_score(pred, y_val)\n",
    "    f1_val = f1_score(pred, y_val, average='weighted')\n",
    "    \n",
    "    acc_sum += acc_val\n",
    "    f1_sum += f1_val\n",
    "    \n",
    "    print (\"Fold:\", fold, \"ACC:\", acc_val, \"F1:\", f1_val)\n",
    "\n",
    "print ()\n",
    "print (\"Avg ACC:\", acc_sum / 4.0, \"Avg F1:\", f1_sum / 4.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% skip\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(model.feature_names_, model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = NGBClassifier(verbose=False)\n",
    "    model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
