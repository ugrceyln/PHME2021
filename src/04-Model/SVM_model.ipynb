{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred import Experiment\n",
    "from sacred.commands import print_config\n",
    "from sacred.observers import FileStorageObserver\n",
    "\n",
    "ex = Experiment('PHME21', interactive=True)\n",
    "# ex.observers.append(FileStorageObserver('./lgbm_logs/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ex.config\n",
    "def configuration_settings():\n",
    "    fill_missing_ = True\n",
    "    scale_type = \"standard\"\n",
    "    window_size_ = 5\n",
    "    train_model = True\n",
    "    eval_model = True\n",
    "    subsample = 10 # for sampling\n",
    "    stride = 5 # for stride\n",
    "    \n",
    "    c_ = 1\n",
    "    kernel_ = 'rbf'\n",
    "    class_weight_ = 'None'\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import data\n",
    "data_df_1 = pd.read_csv(\"../../data/training_validation_1.csv\")\n",
    "data_df_2 = pd.read_csv(\"../../data/training_validation_2.csv\")\n",
    "data_df_3 = pd.read_csv(\"../../data/model_refinement.csv\")\n",
    "\n",
    "train_df = pd.concat([data_df_1, data_df_2, data_df_3], axis=0) # Merge data frames\n",
    "# train_df = data_df_2.filter(regex=\"vCnt|value\")\n",
    "\n",
    "train_df['runId'] = 1000 * train_df['class'] + train_df['run']\n",
    "\n",
    "labels = train_df['class']\n",
    "runs = train_df['runId']\n",
    "\n",
    "run_df = train_df[['class', 'runId']].copy()\n",
    "run_df.drop_duplicates(inplace=True)\n",
    "run_df.reset_index(inplace=True)\n",
    "del run_df['index']\n",
    "\n",
    "# del train_df['class']\n",
    "del train_df['run']\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% skip \n",
    "sensor_list = list(train_df.columns)\n",
    "sensor_list.remove('runId')\n",
    "sensor_list.remove('class')\n",
    "len(sensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list_lgbm = ['CpuTemperature_value',\n",
    " 'DurationPickToPick_vCnt',\n",
    " 'DurationRobotFromFeederToTestBench_vCnt',\n",
    " 'DurationRobotFromFeederToTestBench_value',\n",
    " 'DurationRobotFromTestBenchToFeeder_value',\n",
    " 'DurationTestBenchClosed_vCnt',\n",
    " 'DurationTestBenchClosed_value',\n",
    " 'EPOSCurrent_value',\n",
    " 'EPOSPosition_vCnt',\n",
    " 'EPOSPosition_value',\n",
    " 'EPOSVelocity_vCnt',\n",
    " 'EPOSVelocity_value',\n",
    " 'FeederAction1_vCnt',\n",
    " 'FeederAction2_vCnt',\n",
    " 'FeederAction3_vCnt',\n",
    " 'FeederAction4_vCnt',\n",
    " 'FeederBackgroundIlluminationIntensity_vCnt',\n",
    " 'FuseCycleDuration_vCnt',\n",
    " 'FuseCycleDuration_value',\n",
    " 'FuseHeatSlopeNOK_vCnt',\n",
    " 'FuseHeatSlopeNOK_value',\n",
    " 'FuseHeatSlopeOK_vCnt',\n",
    " 'FuseHeatSlopeOK_value',\n",
    " 'FuseHeatSlope_value',\n",
    " 'FuseIntoFeeder_vCnt',\n",
    " 'FuseOutsideOperationalSpace_vCnt',\n",
    " 'FuseOutsideOperationalSpace_value',\n",
    " 'FusePicked_vCnt',\n",
    " 'FusePicked_value',\n",
    " 'FuseTestResult_vCnt',\n",
    " 'FuseTestResult_value',\n",
    " 'Humidity_value',\n",
    " 'IntensityTotalImage_value',\n",
    " 'IntensityTotalThermoImage_value',\n",
    " 'LightBarrierActiveTaskDuration1_vCnt',\n",
    " 'LightBarrierActiveTaskDuration1_value',\n",
    " 'LightBarrierPassiveTaskDuration1_value',\n",
    " 'NumberFuseDetected_value',\n",
    " 'NumberFuseEstimated_value',\n",
    " 'ProcessCpuLoadNormalized_value',\n",
    " 'ProcessMemoryConsumption_value',\n",
    " 'SharpnessImage_vCnt',\n",
    " 'SharpnessImage_value',\n",
    " 'SmartMotorPositionError_value',\n",
    " 'SmartMotorSpeed_value',\n",
    " 'TemperatureThermoCam_value',\n",
    " 'Temperature_value',\n",
    " 'TotalCpuLoadNormalized_value',\n",
    " 'TotalMemoryConsumption_value',\n",
    " 'VacuumFusePicked_vCnt',\n",
    " 'VacuumFusePicked_value',\n",
    " 'VacuumValveClosed_vCnt',\n",
    " 'VacuumValveClosed_value',\n",
    " 'Vacuum_vCnt',\n",
    " 'Vacuum_value',\n",
    " 'ValidFrameOptrisPIIRCamera_vCnt',\n",
    " 'ValidFrame_vCnt']\n",
    "\n",
    "len(sensor_list_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list_svm = [\n",
    "    'CpuTemperature_value',\n",
    "    'DurationPickToPick_vCnt',\n",
    "    'DurationRobotFromTestBenchToFeeder_value',\n",
    "    'EPOSVelocity_vCnt',\n",
    "    'FeederAction4_vCnt',\n",
    "    'FeederBackgroundIlluminationIntensity_value',\n",
    "    'FuseCycleDuration_value',\n",
    "    'FuseHeatSlopeNOK_value',\n",
    "    'FuseHeatSlope_vCnt',\n",
    "    'Humidity_value',\n",
    "    'IntensityTotalImage_value',\n",
    "    'IntensityTotalThermoImage_value',\n",
    "    'LightBarrieActiveTaskDuration2_vCnt',\n",
    "    'LightBarrierPassiveTaskDuration1_vCnt',\n",
    "    'LightBarrierPassiveTaskDuration1_value',\n",
    "    'LightBarrierPassiveTaskDuration2_vCnt',\n",
    "    'NumberFuseDetected_value',\n",
    "    'ProcessCpuLoadNormalized_value',\n",
    "    'ProcessMemoryConsumption_value',\n",
    "    'SmartMotorPositionError_vCnt',\n",
    "    'SmartMotorSpeed_vCnt',\n",
    "    'TemperatureThermoCam_value',\n",
    "    'Temperature_value',\n",
    "    'TotalCpuLoadNormalized_value',\n",
    "    'TotalMemoryConsumption_value',\n",
    "    'VacuumFusePicked_value',\n",
    "    'ValidFrame_vCnt'\n",
    "]\n",
    "\n",
    "len(sensor_list_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = sensor_list_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from ngboost import NGBClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def create_sequence(sequence, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequence[i:end_ix]\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_run(df, ws):\n",
    "#     data_data = np.empty((0, ws * len(sensor_list))) # for 1D\n",
    "#     data_data = np.empty((0, ws, len(sensor_list))) # for 2D\n",
    "#     data_data = np.empty((0, len(sensor_list), ws)) # for 2D\n",
    "#     label_data = np.empty((0, 1))\n",
    "\n",
    "    sensors_df = df.filter(sensor_list)\n",
    "\n",
    "    # Calculate seq of windows_size len\n",
    "    seq = create_sequence(sensors_df.values, n_steps=ws)\n",
    "#     seq = np.transpose(seq, axes=(0, 2, 1))\n",
    "    seq_count = seq.shape[0]\n",
    "    seq = seq.reshape((seq_count, -1)) # for 1D\n",
    "\n",
    "    seq = seq[::ws//2]\n",
    "    \n",
    "    # add new seq to data_data array\n",
    "#     data_data = np.vstack((data_data, seq))\n",
    "\n",
    "    # Calculate RULS\n",
    "    labels = df['class'].values[:seq_count]\n",
    "\n",
    "    labels = labels[::ws//2]\n",
    "    \n",
    "    # add rul to rul_data array\n",
    "#     rul_data = np.vstack((rul_data, ruls))\n",
    "\n",
    "# TODO: What is RUL_Max in this context?\n",
    "\n",
    "#     print (\"Shape:\", seq.shape, labels.shape)\n",
    "    return seq, labels\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Main sensor list. \n",
    "# List shall include other features such as operating conditions. \n",
    "_subsample = -1\n",
    "_stride = -1\n",
    "\n",
    "# scale_type = \"standard\"\n",
    "# window_size = 50\n",
    "# cv_fold = 3\n",
    "\n",
    "# train_model = True\n",
    "# eval_model = True\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: X_t, X_tp1, y_t, y_tp1 should be calculated per run.  \n",
    "# TODO: Then should be merged into one X_t, X_tp1, y_t, y_tp1.\n",
    "def create_datasets(df, ws):\n",
    "    \n",
    "    run_list = df['runId'].unique()\n",
    "\n",
    "    X_df_list = []\n",
    "    y_df_list = []\n",
    "    \n",
    "    for r in run_list:\n",
    "        r_df = df[df['runId'] == r]\n",
    "#         print (\"--> r: \", r, r_df.shape)\n",
    "        sensor_data, label_data = create_dataset_for_run(r_df, ws)\n",
    "\n",
    "        # Post Processing for the model\n",
    "\n",
    "        # Padding for model input \n",
    "        padded_sensor_data = sensor_data.copy() #np.hstack((sensor_data, np.zeros((sensor_data.shape[0], 2)))) # for AE     \n",
    "\n",
    "        # Calculate X(t) and X(t+1) for model input/output \n",
    "        X_t = padded_sensor_data[:]\n",
    "\n",
    "        # Calculate y(t) and y(t+1) for model input/output \n",
    "        y_t = label_data[:]\n",
    "\n",
    "        X_df_list.append(pd.DataFrame(X_t))\n",
    "        y_df_list.append(pd.DataFrame(y_t))\n",
    "    \n",
    "    X_t = pd.concat(X_df_list, axis=0) # Merge data frames\n",
    "    y_t = pd.concat(y_df_list, axis=0) # Merge data frames\n",
    "\n",
    "    return X_t.values, y_t.values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = StratifiedKFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna_list = [True]\n",
    "ws_list = [1, 5, 10, 15, 20, 25, 30, 40, 50, 100]\n",
    "\n",
    "params_c = [1, 0.1, 0.01, 0.001]\n",
    "params_kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "params_class_weight = ['balanced']\n",
    "\n",
    "total_runs = len(fillna_list) * len(ws_list) * len(params_c) * len(params_kernel)  * len(params_class_weight)\n",
    "\n",
    "total_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna_list = [True]\n",
    "ws_list = [5, 10, 15, 20, 25, 30, 40, 50, 100]\n",
    "\n",
    "params_c = [1, 0.1, 0.01, 0.001]\n",
    "params_kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "params_class_weight = ['balanced']\n",
    "\n",
    "total_runs = len(fillna_list) * len(ws_list) * len(params_c) * len(params_kernel)  * len(params_class_weight)\n",
    "\n",
    "total_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ex.main\n",
    "def ex_main(_run, fill_missing_, scale_type, window_size_, train_model, eval_model, subsample, stride, c_, kernel_, class_weight_):\n",
    "\n",
    "    global total_runs\n",
    "    global run_counter\n",
    "    global results_df\n",
    "    \n",
    "    print ('===========================================================================')\n",
    "    print ('Run:', run_counter+1, '/', total_runs)\n",
    "    print (\"Fill missing:\", fill_missing_, \"Window Size:\", window_size_, \"SVM Params:\", c_, kernel_, class_weight_)\n",
    "\n",
    "    run_counter += 1\n",
    "    \n",
    "    acc_sum = 0\n",
    "    f1_sum = 0\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "    for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "        print (\"--> Fold: \", fold)\n",
    "\n",
    "        training_runIds = run_df.loc[training_indices]['runId']\n",
    "        validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "\n",
    "        X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "        X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "        if (fill_missing_):\n",
    "            X_train_df = X_train_df.interpolate().fillna(method='bfill')\n",
    "            X_val_df = X_val_df.interpolate().fillna(method='bfill')\n",
    "\n",
    "        X_train, y_train = create_datasets(X_train_df, window_size_)\n",
    "        X_val, y_val = create_datasets(X_val_df, window_size_)\n",
    "\n",
    "    #     print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "        print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "        print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "\n",
    "        model = make_pipeline(StandardScaler(), PCA(n_components=53), SVC(C=c_, kernel=kernel_, class_weight=class_weight_, gamma='auto'))        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "#         m = np.argmax(model.feature_importances_)\n",
    "#         print (sensor_list[m], model.feature_importances_[m])\n",
    "\n",
    "        pred = model.predict(X_val)\n",
    "\n",
    "        acc_val = accuracy_score(y_val, pred)\n",
    "        f1_val = f1_score(y_val, pred, average='weighted')\n",
    "\n",
    "        acc_sum += acc_val\n",
    "        f1_sum += f1_val\n",
    "\n",
    "        print (\"Fold:\", fold, \"ACC:\", acc_val, \"F1:\", f1_val)\n",
    "\n",
    "    print ()\n",
    "    print (\"Avg ACC:\", acc_sum / 3.0, \"Avg F1:\", f1_sum / 3.0)\n",
    "\n",
    "    result = {\n",
    "        'ML_Algorithm': 'LGBM',\n",
    "        'Fill missing': fill_missing_, \n",
    "        'Window Size': window_size_, \n",
    "        'Params_C': c_, \n",
    "        'Params_kernel': kernel_, \n",
    "        'Params_class_weight': class_weight_,        \n",
    "        'ACC': acc_sum / 3.0,\n",
    "        'F1': f1_sum / 3.0,\n",
    "    }\n",
    "\n",
    "    results_df = results_df.append(result, ignore_index=True)\n",
    "    results_df.to_excel(\"svm_results.xlsx\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_counter = 0\n",
    "total_runs = len(fillna_list) * len(ws_list) * len(params_c) * len(params_kernel)  * len(params_class_weight)\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "for fnal in fillna_list:\n",
    "    for wsl in ws_list:\n",
    "        for pc in params_c:\n",
    "            for pk in params_kernel:\n",
    "                for pcw in params_class_weight:\n",
    "                    ex.run(config_updates={\n",
    "                        'window_size_': wsl,\n",
    "                        'c_': pc, \n",
    "                        'kernel_': pk,\n",
    "                        'class_weight_': pcw\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_missing_ = True\n",
    "window_size_ = 50\n",
    "c_ = 0.01\n",
    "kernel_ = 'linear'\n",
    "class_weight_ = 'balanced'\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "print ('===========================================================================')\n",
    "print ('Run:', run_counter+1, '/', total_runs)\n",
    "print (\"Fill missing:\", fill_missing_, \"Window Size:\", window_size_, \"SVM Params:\", c_, kernel_, class_weight_)\n",
    "\n",
    "run_counter += 1\n",
    "\n",
    "acc_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for fold, (training_indices, validation_indices) in enumerate(cv.split(run_df['runId'], run_df['class'])):\n",
    "    print (\"--> Fold: \", fold)\n",
    "\n",
    "    training_runIds = run_df.loc[training_indices]['runId']\n",
    "    validation_runIds = run_df.loc[validation_indices]['runId']\n",
    "\n",
    "    X_train_df = train_df[train_df['runId'].isin(training_runIds)].copy()\n",
    "    X_val_df = train_df[train_df['runId'].isin(validation_runIds)].copy()\n",
    "\n",
    "    if (fill_missing_):\n",
    "        X_train_df = X_train_df.interpolate().fillna(method='bfill')\n",
    "        X_val_df = X_val_df.interpolate().fillna(method='bfill')\n",
    "\n",
    "    X_train, y_train = create_datasets(X_train_df, window_size_)\n",
    "    X_val, y_val = create_datasets(X_val_df, window_size_)\n",
    "\n",
    "#     print (\"Data shape\", X_train_df.shape, X_val_df.shape)\n",
    "    print (\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "    print (\"Val data shape:\", X_val.shape, y_val.shape)\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), PCA(n_components=53), SVC(C=c_, kernel=kernel_, class_weight=class_weight_, gamma='auto'))        \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "#         m = np.argmax(model.feature_importances_)\n",
    "#         print (sensor_list[m], model.feature_importances_[m])\n",
    "\n",
    "    for c in train_df['class'].unique():\n",
    "        print (c, f1_score(y_val[y_val == c], model.predict(X_val[y_val == c]), average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_val[y_val == 11])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in train_df['class'].unique():\n",
    "    print (c, f1_score(y_val[y_val == c], model.predict(X_val[y_val == c]), average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([np.nan, 1, 2, 3, np.nan]).interpolate().fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
