{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TEST PERFORMACNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_files():\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_fields_dict.pickle\", \"rb\") as input_file:\n",
    "        fields_dict = pickle.load(input_file)\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_sensor_list.pickle\", \"rb\") as input_file:\n",
    "        sensor_list = pickle.load(input_file)\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_scaler.pickle\", \"rb\") as input_file:\n",
    "        scaler = pickle.load(input_file)\n",
    "        \n",
    "    with open(\"../../data/results/models/_sub_lda.pickle\", \"rb\") as input_file:\n",
    "        lda = pickle.load(input_file)\n",
    "        \n",
    "    with open(\"../../data/results/models/_sub_model_xgb.pickle\", \"rb\") as input_file:\n",
    "        model = pickle.load(input_file)\n",
    "        \n",
    "    return fields_dict, sensor_list, scaler, lda, model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class id and run id from filename\n",
    "def parse_class_name(fname):\n",
    "    p = re.compile(\"^class[^\\d]*(\\d+)_(\\d+).*.csv\")\n",
    "    m = p.match(fname)\n",
    "\n",
    "    return m.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one data file and return in a data frame\n",
    "def load_data_file(path, fname):\n",
    "    \n",
    "    fname = \"{}.csv\".format(fname)\n",
    "    fullpath = join(path, fname)\n",
    "    df = pd.read_csv(fullpath)\n",
    "    df.columns = ['name', 'data']\n",
    "\n",
    "    dfx = []\n",
    "\n",
    "    for f in fields_dict:\n",
    "        name = fields_dict[f]['name']\n",
    "        fields = fields_dict[f]['fields']\n",
    "\n",
    "        data = eval(df.loc[f, 'data'])  # convert data to array\n",
    "\n",
    "        new_df = pd.DataFrame(data)\n",
    "        if (f == 33) and (new_df.shape[1] == 6):  # NumberFuseDetected has a special case!\n",
    "            new_df[6] = new_df[5]\n",
    "            new_df[5] = np.NaN\n",
    "\n",
    "        new_df.columns = fields_dict[f]['fields']\n",
    "\n",
    "        dfx.append(new_df)\n",
    "\n",
    "    merged_df = pd.concat(dfx, axis=1)  # Merge columns\n",
    "\n",
    "    # # Do some imputation on the data file\n",
    "    # merged_df = impute_df(merged_df.copy())\n",
    "\n",
    "    c, r = parse_class_name(fname)  # Get class id and run id\n",
    "\n",
    "    # Add class labels and run id\n",
    "    merged_df['class'] = int(c)\n",
    "    merged_df['run'] = int(r)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_values(data, name, fields):\n",
    "\n",
    "    field_df = data[fields]\n",
    "\n",
    "    if field_df.isnull().values.any():\n",
    "        data[fields] = field_df.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return data[fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a sequence into samples\n",
    "def create_sequence(sequence, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequence[i:end_ix]\n",
    "        X.append(seq_x)\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "def create_dataset_for_run(df, ws):\n",
    "    #     data_data = np.empty((0, ws * len(sensor_list))) # for 1D\n",
    "    #     data_data = np.empty((0, ws, len(sensor_list))) # for 2D\n",
    "    #     data_data = np.empty((0, len(sensor_list), ws)) # for 2D\n",
    "    #     label_data = np.empty((0, 1))\n",
    "\n",
    "    sensors_df = df.filter(sensor_list)\n",
    "\n",
    "    # Calculate seq of windows_size len\n",
    "    seq = create_sequence(sensors_df.values, n_steps=ws)\n",
    "    #     seq = np.transpose(seq, axes=(0, 2, 1))\n",
    "    seq_count = seq.shape[0]\n",
    "    seq = seq.reshape((seq_count, -1))  # for 1D\n",
    "\n",
    "    # add new seq to data_data array\n",
    "    # data_data = np.vstack((data_data, seq))\n",
    "\n",
    "    # Calculate RULS\n",
    "    labels = df['class'].values[:seq_count]\n",
    "\n",
    "    # add rul to rul_data array\n",
    "    #     rul_data = np.vstack((rul_data, ruls))\n",
    "\n",
    "    # TODO: What is RUL_Max in this context?\n",
    "\n",
    "    # print (\"Shape:\", seq.shape, labels.shape)\n",
    "\n",
    "    return seq, labels\n",
    "\n",
    "# TODO: X_t, X_tp1, y_t, y_tp1 should be calculated per run.\n",
    "# TODO: Then should be merged into one X_t, X_tp1, y_t, y_tp1.\n",
    "\n",
    "def create_datasets(df, ws):\n",
    "    run_list = df['runId'].unique()\n",
    "    l_len_runs = []\n",
    "\n",
    "    X_df_list = []\n",
    "    y_df_list = []\n",
    "\n",
    "    for r in run_list:\n",
    "        r_df = df[df['runId'] == r]\n",
    "        # print (\"--> r: \", r, r_df.shape)\n",
    "        sensor_data, label_data = create_dataset_for_run(r_df, ws)\n",
    "\n",
    "        # Post Processing for the model\n",
    "\n",
    "        # Padding for model input\n",
    "        padded_sensor_data = sensor_data.copy()  # np.hstack((sensor_data, np.zeros((sensor_data.shape[0], 2)))) # for AE\n",
    "\n",
    "        # Calculate X(t) and X(t+1) for model input/output\n",
    "        X_t = padded_sensor_data[:]\n",
    "\n",
    "        # Calculate y(t) and y(t+1) for model input/output\n",
    "        y_t = label_data[:]\n",
    "\n",
    "        X_df_list.append(pd.DataFrame(X_t))\n",
    "        y_df_list.append(pd.DataFrame(y_t))\n",
    "        l_len_runs.append(len(X_t))\n",
    "\n",
    "    X_t = pd.concat(X_df_list, axis=0)  # Merge data frames\n",
    "    y_t = pd.concat(y_df_list, axis=0)  # Merge data frames\n",
    "\n",
    "    return X_t.values, y_t.values.flatten(), run_list, l_len_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimum_point_degradation(y_pred, ws):\n",
    "    \n",
    "    results_dict = Counter(y_pred)\n",
    "    \n",
    "    ws_step = 5\n",
    "\n",
    "    most_common = dict(results_dict.most_common(1))\n",
    "    most_commons = dict(results_dict.most_common(2))\n",
    "    \n",
    "    true_class = list(most_common.keys())[0]\n",
    "    # print(\"most_commons:\", most_commons)\n",
    "    \n",
    "    if len(most_commons) == len(most_common): # if only one class\n",
    "        return true_class, ws + ws_step\n",
    "            \n",
    "    l_optimum = []\n",
    "    \n",
    "    for i in range(0, len(y_pred), ws_step):\n",
    "        x_steps = y_pred[i: i+ws_step]\n",
    "        l_optimum.extend(list(x_steps))\n",
    "        # print(i, i + ws_step, list(x_steps), len(l_optimum))\n",
    "        \n",
    "        results_dict_ = Counter(l_optimum)\n",
    "    \n",
    "        most_common2 = dict(results_dict_.most_common(1))\n",
    "        pred_class = list(most_common2.keys())[0]\n",
    "        \n",
    "        if pred_class == true_class:\n",
    "            \n",
    "            return true_class, ws + i + ws_step                          \n",
    "        \n",
    "    # print(\"burda!!!\")      \n",
    "    return true_class, len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_class, opt_cut_point = get_optimum_point_degradation(run_data, ws)\n",
    "# true_class, opt_cut_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_class, opt_cut_point = get_optimum_point_degradation(run_data[:opt_cut_point], ws)\n",
    "# true_class, opt_cut_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ws = 40\n",
    "# x_data = pd.read_excel(\"../../data/results/excels/xgb_ws_40_fold_1_mcc.xlsx\", index_col=False)\n",
    "# x_data = x_data[[\"Unnamed: 0\", \"pred\"]]\n",
    "# run_data = x_data.loc[x_data[\"Unnamed: 0\"] == 25][\"pred\"]\n",
    "# run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(run_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rank_class(label):\n",
    "    \n",
    "    if label == 0:\n",
    "        return [\"S4\",\"S2\",\"S1\",\"S3\"]\n",
    "    elif label == 2:\n",
    "        return [\"S3\",\"S4\",\"S1\",\"S3\"]\n",
    "    # ...\n",
    "    \n",
    "    else:\n",
    "        return [\"S1\",\"S2\",\"S4\",\"S3\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Classification is the prototype of the function that each team must develop to classify new data\n",
    "#This function must handle all the operation to: read in a streaming order the input file, make the earlier possibile classification, return the required data\n",
    "#Input: \n",
    "# - Folder Name: The name of the folder where the experiment file is stored\n",
    "# - Experiment: The experiment name that must be read \n",
    "#Output:\n",
    "# - Predicted Label: the label predicted by the classifier\n",
    "# - Time for classification: how much time of the input data was required to perform the classification task\n",
    "# - Ranking: The Features ranked according to the team solution\n",
    "\n",
    "def TestClassification(FolderName, Experiment):\n",
    "          \n",
    "    Label = \"\"\n",
    "    Time = -1\n",
    "    Ranking = []\n",
    "    \n",
    "    \n",
    "    ws = 40\n",
    "    \n",
    "    fields_dict, sensor_list, scaler, lda, model = load_pickle_files()\n",
    "    \n",
    "    df = load_data_file(FolderName, Experiment)\n",
    "    # print(df.isnull().sum().any())\n",
    "    \n",
    "    for f in fields_dict:\n",
    "        name = fields_dict[f]['name']\n",
    "        fields = fields_dict[f]['fields']\n",
    "\n",
    "        # print(\"\\nname:\", name, \"fields:\", fields)\n",
    "        df_ = df.groupby([\"class\", \"run\"]).apply(fill_nan_values, name, fields)\n",
    "        df_.reset_index(drop=True, inplace=True)\n",
    "        df[fields] = df_[fields]\n",
    "        \n",
    "    # print(df.isnull().sum().any())\n",
    "    \n",
    "    # for column in df.columns:\n",
    "    #     if column not in [\"class\", \"run\"]:\n",
    "    #         if (len(df[column].unique()) == 1) or (df[column].isnull().all()):\n",
    "    #             df.drop(column, inplace=True, axis=1)\n",
    "    #             print(column, \"droped-unique\")\n",
    "    # \n",
    "    #         else:\n",
    "    #             zero_rows = df.loc[df[column] == float(0)]\n",
    "    #             if zero_rows.shape[0] >= df.shape[0] * 50:\n",
    "    #                 df.drop(column, inplace=True, axis=1)\n",
    "    #                 print(column, \"droped-zero\")\n",
    "    # \n",
    "    # print(df.isnull().sum().any()) \n",
    "    # print(df.shape)\n",
    "\n",
    "    df = df[sensor_list + [\"class\", \"run\"]]\n",
    "    df = df.rename(columns={'run': 'runId'})\n",
    "    \n",
    "    X_test_df = df[sensor_list + [\"class\", \"runId\"]].copy()\n",
    "    \n",
    "    scaler_cols = sensor_list.copy()  # list(set(sensor_list).difference([\"class\", \"runId\"]))\n",
    "\n",
    "    scaler_data_ts = scaler.transform(X_test_df[scaler_cols])\n",
    "    scaler_data_ts = pd.DataFrame(scaler_data_ts, index=X_test_df.index, columns=scaler_cols)\n",
    "    X_test_df = pd.concat([X_test_df[[\"class\", \"runId\"]], scaler_data_ts], axis=1)\n",
    "\n",
    "    X_test, y_test, runList_test, l_len_runs_test = create_datasets(X_test_df, ws)\n",
    "    X_test = lda.transform(X_test)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #Outputs\n",
    "    Label, Time = get_optimum_point_degradation(y_pred, ws)\n",
    "    Ranking = get_rank_class(Label)\n",
    "\n",
    "    return Label, Time, Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Record Performance function store the information of the performance achieved in the 1st run of the classification\n",
    "def RecordPerformance(Experiment, Label, Time, Ranking):\n",
    "\n",
    "    if not os.path.exists('First'):\n",
    "        os.makedirs('First')\n",
    "    \n",
    "    PerformanceOutput = open(\"First/%s.csv\"%Experiment,\"w\")\n",
    "    PerformanceOutput.write(\"Experiment;Label;Time;Ranking\\n\")\n",
    "    PerformanceOutput.write(Experiment+\";\"+str(Label)+\";\"+str(Time)+\";\"+str(Ranking)+\"\\n\")\n",
    "    PerformanceOutput.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "#The CutExperiment function is used to cut the input experiment in the time order. For each experiment, the cut is performed accoring to the time to classification declared by the team \n",
    "def CutExperiment(FolderName, Experiment, Time):\n",
    "\n",
    "    if not os.path.exists('Cut'):\n",
    "        os.makedirs('Cut')\n",
    "    \n",
    "    data = pd.read_csv(FolderName+\"/%s.csv\"%Experiment,sep=\",\")  \n",
    "\n",
    "    df = pd.DataFrame(columns = [\"c1\",\"c2\"])\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        field = data.iloc[i][0]\n",
    "        records = eval(data.iloc[i][1])[:Time]\n",
    "\n",
    "        df = df.append({\"c1\": field, \"c2\": records}, ignore_index=True)    \n",
    "    \n",
    "    columns =list(data.columns)\n",
    "    df.columns = [\"\",columns[1]]\n",
    "    df.to_csv(\"Cut/%s.csv\"%Experiment,index=False)\n",
    "    return\n",
    "\n",
    "#ComparePerfomance check if the team achieved the same performance in the 1st and in the 2nd run \n",
    "def ComparePerformance(Experiment,Label, Time, Ranking):\n",
    "    \n",
    "    Performance = pd.read_csv(\"First/%s.csv\"%Experiment,sep=\";\")        \n",
    "        \n",
    "    if(Performance[\"Label\"].iloc[0]!=Label): return False\n",
    "    if(Performance[\"Time\"].iloc[0]!=Time): return False\n",
    "    if(Performance[\"Ranking\"].iloc[0]!=str(Ranking)): return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "#GetWorst function returns the worst performance in case the 1st and 2nd run performance does not match\n",
    "def GetWorst(Experiment, Label, Time, Ranking):\n",
    "    \n",
    "    Performance = pd.read_csv(\"First/%s.csv\"%Experiment,sep=\";\")  \n",
    "    \n",
    "    if(Performance[\"Label\"].iloc[0]!=Label): return Performance[\"Label\"].iloc[0], -1, Performance[\"Ranking\"].iloc[0]\n",
    "    if(Performance[\"Time\"].iloc[0]!=Time): return Performance[\"Label\"].iloc[0], Performance[\"Time\"].iloc[0], Performance[\"Ranking\"].iloc[0]\n",
    "    if(Performance[\"Ranking\"].iloc[0]!=str(Ranking)): return Performance[\"Label\"].iloc[0], Performance[\"Time\"].iloc[0], Performance[\"Ranking\"].iloc[0]\n",
    "    \n",
    "    return\n",
    "\n",
    "#Logperformance function stores the final performance. Only this performance will be used to compute the Penalty score of each team\n",
    "def LogPerformance(Experiment,Label, Time, Ranking):\n",
    "\n",
    "    if not os.path.exists('Results'):\n",
    "        os.makedirs('Results')\n",
    "        \n",
    "    PerformanceOutput = open(\"Results/%s.csv\"%Experiment,\"w\")\n",
    "    PerformanceOutput.write(\"Experiment;Label;Time;Ranking\\n\")\n",
    "    PerformanceOutput.write(Experiment+\";\"+str(Label)+\";\"+str(Time)+\";\"+str(Ranking)+\"\\n\")\n",
    "    PerformanceOutput.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(361, 249)\n",
      "most_commons: {0: 285, 5: 20}\n",
      "0 5 [11, 11, 11, 11, 11] 5\n",
      "5 10 [11, 11, 11, 11, 11] 10\n",
      "10 15 [11, 11, 11, 11, 11] 15\n",
      "15 20 [11, 5, 5, 5, 5] 20\n",
      "20 25 [5, 5, 5, 5, 5] 25\n",
      "25 30 [5, 5, 5, 5, 5] 30\n",
      "30 35 [5, 5, 5, 5, 5] 35\n",
      "35 40 [5, 2, 0, 0, 0] 40\n",
      "40 45 [0, 0, 0, 0, 0] 45\n",
      "45 50 [0, 0, 0, 0, 0] 50\n",
      "50 55 [0, 0, 0, 0, 0] 55\n",
      "55 60 [0, 0, 0, 0, 0] 60\n",
      "0 100 ['S4', 'S2', 'S1', 'S3']\n",
      "(100, 249)\n",
      "most_commons: {0: 64, 5: 20}\n",
      "0 5 [11, 11, 11, 11, 11] 5\n",
      "5 10 [11, 11, 11, 11, 11] 10\n",
      "10 15 [11, 11, 11, 11, 11] 15\n",
      "15 20 [11, 5, 5, 5, 5] 20\n",
      "20 25 [5, 5, 5, 5, 5] 25\n",
      "25 30 [5, 5, 5, 5, 5] 30\n",
      "30 35 [5, 5, 5, 5, 5] 35\n",
      "35 40 [5, 2, 0, 0, 0] 40\n",
      "40 45 [0, 0, 0, 0, 0] 45\n",
      "45 50 [0, 0, 0, 0, 0] 50\n",
      "50 55 [0, 0, 0, 0, 0] 55\n",
      "55 60 [0, 0, 0, 0, 0] 60\n",
      "0 100 ['S4', 'S2', 'S1', 'S3']\n",
      "Equal: True\n"
     ]
    }
   ],
   "source": [
    "#Example of the validation pipleline by using a single experiment.\n",
    "#Data/ is the folder where the experiment is stored\n",
    "#class_0_0_data is the experiment name\n",
    "#Cut/ is the folder where only the cut experiment will be saved\n",
    "\n",
    "def main():\n",
    "    \n",
    "    FolderName = \"Data/\"\n",
    "    Experiment = \"class_ 0_0_data\"\n",
    "    Label, Time, Ranking = TestClassification(FolderName, Experiment)\n",
    "    \n",
    "    RecordPerformance(Experiment, Label, Time, Ranking)\n",
    "    CutExperiment(FolderName,Experiment,Time)\n",
    "     \n",
    "    FolderName = \"Cut/\"\n",
    "    Label, Time, Ranking = TestClassification(FolderName,Experiment)\n",
    "    \n",
    "    Equal = ComparePerformance(Experiment,Label, Time, Ranking)\n",
    "    if(Equal==False):\n",
    "        Label, Time, Ranking = GetWorst(Experiment,Label, Time, Ranking)\n",
    "     \n",
    "    LogPerformance(Experiment,Label, Time, Ranking)\n",
    "    return \n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle_files2():\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_fields_dict.pickle\", \"rb\") as input_file:\n",
    "        fields_dict = pickle.load(input_file)\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_sensor_list.pickle\", \"rb\") as input_file:\n",
    "        sensor_list = pickle.load(input_file)\n",
    "    \n",
    "    with open(\"../../data/results/models/_sub_scaler.pickle\", \"rb\") as input_file:\n",
    "        scaler = pickle.load(input_file)\n",
    "        \n",
    "    with open(\"../../data/results/models/_sub_model_kmeans.pickle\", \"rb\") as input_file:\n",
    "        model = pickle.load(input_file)\n",
    "        \n",
    "    return fields_dict, sensor_list, scaler, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one data file and return in a data frame\n",
    "def load_data_file2(path, fname):\n",
    "    \n",
    "    fullpath = join(path, fname)\n",
    "    df = pd.read_csv(fullpath)\n",
    "    df.columns = ['name', 'data']\n",
    "\n",
    "    dfx = []\n",
    "\n",
    "    for f in fields_dict:\n",
    "        name = fields_dict[f]['name']\n",
    "        fields = fields_dict[f]['fields']\n",
    "\n",
    "        data = eval(df.loc[f, 'data'])  # convert data to array\n",
    "\n",
    "        new_df = pd.DataFrame(data)\n",
    "        if (f == 33) and (new_df.shape[1] == 6):  # NumberFuseDetected has a special case!\n",
    "            new_df[6] = new_df[5]\n",
    "            new_df[5] = np.NaN\n",
    "\n",
    "        new_df.columns = fields_dict[f]['fields']\n",
    "\n",
    "        dfx.append(new_df)\n",
    "\n",
    "    merged_df = pd.concat(dfx, axis=1)  # Merge columns\n",
    "\n",
    "    # # Do some imputation on the data file\n",
    "    # merged_df = impute_df(merged_df.copy())\n",
    "\n",
    "    c, r = parse_class_name(fname)  # Get class id and run id\n",
    "\n",
    "    # Add class labels and run id\n",
    "    merged_df['class'] = int(c)\n",
    "    merged_df['run'] = int(r)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BONUS POINT: This script is used to assess the performance of the clustering result\n",
    "#Test CreateCluster is the prototype of the function that each team can develop to cluster fault-free experiments\n",
    "#This function must handle all the operation to: read the input files and return the clustering result\n",
    "#Input: \n",
    "# - Folder Name: The name of the folder where the experiment file are stored\n",
    "#Output:\n",
    "# - ExperimentList: the name of the exeperiments in the input Folder. \n",
    "### IMPORTANT: This list must return the experiment in the same order as processed by the clustering. \n",
    "# - ClusterLabels: The cluster ID for each Experiment in the ExperimentList list\n",
    "\n",
    "def CreateCluster(FolderName):\n",
    "\n",
    "    ExperimentList = [f for f in listdir(FolderName) if isfile(join(FolderName, f))]\n",
    "    \n",
    "    ClusterLabels = []\n",
    "    \n",
    "    ws = 40    \n",
    "    fields_dict, sensor_list, scaler, model = load_pickle_files2()\n",
    "    \n",
    "    for Experiment in ExperimentList:\n",
    "        \n",
    "        df = load_data_file2(FolderName, Experiment)\n",
    "        # print(df.isnull().sum().any())\n",
    "\n",
    "        for f in fields_dict:\n",
    "            name = fields_dict[f]['name']\n",
    "            fields = fields_dict[f]['fields']\n",
    "\n",
    "            # print(\"\\nname:\", name, \"fields:\", fields)\n",
    "            df_ = df.groupby([\"class\", \"run\"]).apply(fill_nan_values, name, fields)\n",
    "            df_.reset_index(drop=True, inplace=True)\n",
    "            df[fields] = df_[fields]\n",
    "\n",
    "        # print(df.isnull().sum().any())\n",
    "\n",
    "        # for column in df.columns:\n",
    "        #     if column not in [\"class\", \"run\"]:\n",
    "        #         if (len(df[column].unique()) == 1) or (df[column].isnull().all()):\n",
    "        #             df.drop(column, inplace=True, axis=1)\n",
    "        #             print(column, \"droped-unique\")\n",
    "        # \n",
    "        #         else:\n",
    "        #             zero_rows = df.loc[df[column] == float(0)]\n",
    "        #             if zero_rows.shape[0] >= df.shape[0] * 50:\n",
    "        #                 df.drop(column, inplace=True, axis=1)\n",
    "        #                 print(column, \"droped-zero\")\n",
    "        # \n",
    "        # print(df.isnull().sum().any()) \n",
    "        # print(df.shape)\n",
    "\n",
    "        df = df[sensor_list + [\"class\", \"run\"]]\n",
    "        df = df.rename(columns={'run': 'runId'})\n",
    "\n",
    "        X_test_df = df[sensor_list + [\"class\", \"runId\"]].copy()\n",
    "\n",
    "        scaler_cols = sensor_list.copy()  # list(set(sensor_list).difference([\"class\", \"runId\"]))\n",
    "        # scaler_cols = ['Temperature_value', 'Humidity_value']\n",
    "\n",
    "        scaler_data_ts = scaler.transform(X_test_df[scaler_cols])\n",
    "        scaler_data_ts = pd.DataFrame(scaler_data_ts, index=X_test_df.index, columns=scaler_cols)\n",
    "        X_test = scaler_data_ts[['Temperature_value', 'Humidity_value']]\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        results_dict = Counter(y_pred)\n",
    "        \n",
    "        most_common = dict(results_dict.most_common(1))\n",
    "        true_class = list(most_common.keys())[0]\n",
    "        \n",
    "        ClusterLabels.append(true_class)        \n",
    "    \n",
    "\n",
    "    return ExperimentList, ClusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logperformance function stores the final performance. Only this performance will be used to compute the Penalty score of each team\n",
    "def LogPerformance(ExperimentList, ClusterLabels):\n",
    "\n",
    "    if not os.path.exists('Cluster_Results'):\n",
    "        os.makedirs('Cluster_Results')\n",
    "        \n",
    "    PerformanceOutput = open(\"Cluster_Results/ClusterPerformance.csv\",\"w\")\n",
    "    PerformanceOutput.write(\"Experiment;ClusterLabel\\n\")\n",
    "    for i in range(len(ExperimentList)):\n",
    "        Experiment   = ExperimentList[i]\n",
    "        ClusterLabel = ClusterLabels[i]\n",
    "        PerformanceOutput.write(Experiment+\";\"+str(ClusterLabel)+\"\\n\")\n",
    "    PerformanceOutput.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_ 0_0_data.csv', 'class_ 0_100_data.csv', 'class_ 0_10_data.csv', 'class_ 0_13_data.csv', 'class_ 0_26_data.csv', 'class_ 0_28_data.csv', 'class_ 0_42_data.csv', 'class_ 0_44_data.csv', 'class_ 0_49_data.csv', 'class_ 0_51_data.csv', 'class_ 0_58_data.csv', 'class_ 0_61_data.csv', 'class_ 0_64_data.csv', 'class_ 0_69_data.csv', 'class_ 0_73_data.csv', 'class_ 0_82_data.csv', 'class_ 0_89_data.csv', 'class_ 0_92_data.csv', 'class_ 0_94_data.csv', 'class_ 0_99_data.csv'] [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Example of the validation pipleline for the BONUS POINT.\n",
    "#Data/ is the folder where the experiment is stored\n",
    "\n",
    "def main2():\n",
    "    \n",
    "    FolderName = \"Data/\"\n",
    "    ExperimentList, ClusterLabels = CreateCluster(FolderName)\n",
    "    print(ExperimentList, ClusterLabels)\n",
    "    \n",
    "    LogPerformance(ExperimentList, ClusterLabels)\n",
    "    return\n",
    "\n",
    "main2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (EnrichProjects)",
   "language": "python",
   "name": "pycharm-b64b4bc7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
